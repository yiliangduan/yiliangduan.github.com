<!DOCTYPE html>













<html class="theme-next mist" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222"/>
























<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2"/>

<link rel="stylesheet" href="/css/main.css?v=7.1.0"/>


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.1.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/favicon.ico?v=7.1.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/favicon.ico?v=7.1.0">


  <link rel="mask-icon" href="/images/logo.svg?v=7.1.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '7.1.0',
    sidebar: {"position":"left","display":"remove","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="这是阅读RealTime-Rendering 4th Edition的笔记，为了便于自己的理解直接一边阅读一边根据自己理解翻译下来，记录在这里。  这章介绍实时渲染的核心部分，也就是图形渲染管线(graphics rendering pipeline)，也称为“管线”。它主要的功能是将一个虚拟的摄像机，三维的对象，光源等这些场景元素生成或者渲染成一张二位的图片。因此，渲染管线是实时渲染的基础工具">
<meta name="keywords" content="graphics">
<meta property="og:type" content="article">
<meta property="og:title" content="RTR4 第二章 图形渲染管线">
<meta property="og:url" content="http://yiliangduan.github.com/2019/05/06/rtr4/chapter2_the_graphics_rendering_pipeline/index.html">
<meta property="og:site_name" content="一点心得">
<meta property="og:description" content="这是阅读RealTime-Rendering 4th Edition的笔记，为了便于自己的理解直接一边阅读一边根据自己理解翻译下来，记录在这里。  这章介绍实时渲染的核心部分，也就是图形渲染管线(graphics rendering pipeline)，也称为“管线”。它主要的功能是将一个虚拟的摄像机，三维的对象，光源等这些场景元素生成或者渲染成一张二位的图片。因此，渲染管线是实时渲染的基础工具">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yiliangduan.github.com/images/realtime_rendering/chapter2/figure2.1.png">
<meta property="og:image" content="http://yiliangduan.github.com/images/realtime_rendering/chapter2/figure2.2.png">
<meta property="og:image" content="http://yiliangduan.github.com/images/realtime_rendering/chapter2/figure2.3.png">
<meta property="og:image" content="http://yiliangduan.github.com/images/realtime_rendering/chapter2/figure2.4.png">
<meta property="og:image" content="http://yiliangduan.github.com/images/realtime_rendering/chapter2/figure2.5.png">
<meta property="og:image" content="http://yiliangduan.github.com/images/realtime_rendering/chapter2/other2.1.png">
<meta property="og:image" content="http://yiliangduan.github.com/images/realtime_rendering/chapter2/other2.2.png">
<meta property="og:image" content="http://yiliangduan.github.com/images/realtime_rendering/chapter2/figure2.6.png">
<meta property="og:image" content="http://yiliangduan.github.com/images/realtime_rendering/chapter2/figure2.7.png">
<meta property="og:image" content="http://yiliangduan.github.com/images/realtime_rendering/chapter2/figure2.8.png">
<meta property="og:image" content="http://yiliangduan.github.com/images/realtime_rendering/chapter2/figure2.9.png">
<meta property="og:updated_time" content="2019-05-06T02:27:24.866Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="RTR4 第二章 图形渲染管线">
<meta name="twitter:description" content="这是阅读RealTime-Rendering 4th Edition的笔记，为了便于自己的理解直接一边阅读一边根据自己理解翻译下来，记录在这里。  这章介绍实时渲染的核心部分，也就是图形渲染管线(graphics rendering pipeline)，也称为“管线”。它主要的功能是将一个虚拟的摄像机，三维的对象，光源等这些场景元素生成或者渲染成一张二位的图片。因此，渲染管线是实时渲染的基础工具">
<meta name="twitter:image" content="http://yiliangduan.github.com/images/realtime_rendering/chapter2/figure2.1.png">





  
  
  <link rel="canonical" href="http://yiliangduan.github.com/2019/05/06/rtr4/chapter2_the_graphics_rendering_pipeline/"/>



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>RTR4 第二章 图形渲染管线 | 一点心得</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        
        <span class="site-title">一点心得</span>
        
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br/>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br/>归档</a>

  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-tags" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yiliangduan.github.com/2019/05/06/rtr4/chapter2_the_graphics_rendering_pipeline/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yiliangduan@gmail.com"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="一点心得"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">RTR4 第二章 图形渲染管线

              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-05-06 09:49:40 / Modified: 10:27:24" itemprop="dateCreated datePublished" datetime="2019-05-06T09:49:40+08:00">2019-05-06</time>
            

            
              

              
            
          </span>

          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <blockquote>
<p>这是阅读RealTime-Rendering 4th Edition的笔记，为了便于自己的理解直接一边阅读一边根据自己理解翻译下来，记录在这里。</p>
</blockquote>
<p>这章介绍实时渲染的核心部分，也就是<em>图形渲染管线(graphics rendering pipeline)</em>，也称为“管线”。它主要的功能是将一个虚拟的摄像机，三维的对象，光源等这些场景元素生成或者渲染成一张二位的图片。因此，渲染管线是实时渲染的基础工具。使用管线的处理如图2.1所示。在生成图片中的对象的位置和形状是由这些对象的几何结构、环境特征和摄像机放在环境中的位置决定的。对象的外观受材质的属性、光源、贴图(图片应用到面)和着色的方程所影响。</p>
<a id="more"></a> 
<p><img src="/images/realtime_rendering/chapter2/figure2.1.png" alt=""></p>
<blockquote>
<p>图2.1.  在左边的图片中，一个虚拟的摄像机位于金字塔的最顶端（在四条直线聚集的地方）。只有那些在可视空间体（view volume）内的基元（译:基元指最小的渲染单位，包括点、线和三角形）才会被渲染。对于以透视渲染的图片（如这里的情况），可视空间体是一个平截的视椎体，比如金字塔的矩形底。右边的图片展示了摄像机所看到的内容。注意到左图中的环状红色的圈没有渲染到有图中，因为它的位置在可视空间体的外部了。而且，左图中这个扭曲的蓝色的菱柱被可视空间体的顶部面片裁剪。</p>
</blockquote>
<p>我们将更加集中解释渲染光线在这两个不同的阶段的功能而不是渲染的原理，这些阶段的详细相关应用将在后面的章节介绍。</p>
<h4 id="2-1-架构"><a href="#2-1-架构" class="headerlink" title="2.1 架构"></a>2.1 架构</h4><p>在物理世界中，管线的概念表现为许多不同的形式，从组装工厂到快餐厨房。当然它也应用到图形渲染。一个管线由多个阶段组成，每个阶段都执行了大规模任务的其中一部分。</p>
<p>管线的阶段是并行执行的，每个阶段都依赖于上一个阶段的结果。理想情况下，将非管线系统分成<em>n</em>个管线阶段能够提高<em>n</em>倍速度。提高的性能主要原因来自使用了管线。举个例子，一系列的人可以快速的做出大量的三明治，一个人准备面包，另外一个人添加肉，再让一个人添加浇头。每个人处理好一个自己的一步之后马上传递给这条线上的接下来的那个人，随即开始处理下一个三明治。如果每个人花费20秒钟去处理他们的任务，做出一个三明治的最高速度为20秒，每分钟做出三个三明治。管线的每个阶段的执行都是并行的，但是他们处理好自己的任务之后暂停住，直到最慢的那个阶段完成。比如，让添加肉的阶段变的更加复杂点，需要花费30秒。现在每分钟最快的速度能完成两个三明治。对于这个制作三明治的管线，添加肉的阶段是性能的瓶颈，因为它决定了整个生产的速度。添加浇头的阶段等待添加肉阶段完成的这段时间叫<em>饥饿</em>(也叫消费者)时间。</p>
<p>这种结构类型的管线也存在于实时计算机图形中。实时渲染管线粗略的划分为四个主要的阶段—应用层，几何处理，光栅化和像素处理，如图2.2所示。这个结构是渲染管线引擎的核心，它被用在实时计算机渲染应用程序中因此这是接下来的章节讨论的必不可少的基础。</p>
<p><img src="/images/realtime_rendering/chapter2/figure2.2.png" alt=""></p>
<blockquote>
<p>图2.2. 渲染光线基础结构，由四阶段组成: 应用层（Application），几何处理（Geometry Processing），光栅化（Rasterization）和像素处理（Pixel Processing）。每个阶段本身可能是一个管线，如示意图下的几何处理阶段，或者是一个阶段（部分）其本身可能是并行的，如示意图下展示的像素处理阶段。在这个示意图中，应用阶段是单任务处理的，但是这个阶段也可以是管线化或者并行化的。注意到光栅化之后使得渲染的像素在一个基元（如三角形）当中。</p>
</blockquote>
<h4 id="2-2-应用阶段"><a href="#2-2-应用阶段" class="headerlink" title="2.2 应用阶段"></a>2.2 应用阶段</h4><p>开发者可以完全控制应用层发生的事情，因为应用层的任务通常是在CPU中执行的。因此，开发者为了提高性能可以完全决定实现方案并且之后可以对方案进行修改。对这个阶段的修改也会影响之后的阶段的性能，例如，应用层阶段的算法或者设置能够减少渲染三角形的数量。</p>
<p>此外，使用一种叫做<em>计算机着色器（computer shader）</em>的分离模式下，一些应用层的任务可以让GPU来执行。这种模式下把GPU作为一个高度并行综合处理器，忽略它是专门用来作为渲染图形的特殊功能。</p>
<p>在应用层阶段处理的最后，会将需要渲染的几何数据传到几何处理阶段。这些几何数据就是<em>渲染基元</em>，比如，点、线和三角形，这些都是最终可能显示在屏幕上的（或者是无论如何输出设备都需要用到的）。这时应用层阶段最终的的任务。</p>
<p>这个阶段基于软件实现的结果是它没有像几何处理、光栅化和像素处理阶段一样被分成多个子阶段。然而，为了提高性能，这个阶段经常在多个处理器核心上并行的执行。在CPU设计中，这叫做<em>超标量(superscalar)</em>结构，因为他可以能够在同一个阶段同一时间处理多个进程。在18.5节中展示了使用多处理器核心的方法。</p>
<p>通常在这个阶段实现一个叫做<em>碰撞检测（collision detection）</em>的一个任务。在两个对象进行了碰撞检测之后，可以生成一个响应并发送回给碰撞的对象和力的反馈装置。应用层阶段也是处理其他输入源的地方，比如键盘，鼠标或者是头戴显示设备。根据这些输入可以采取各种不同的操作。加速算法，比如特殊的裁剪算法(19章节)也是在这一阶段实现的，这是管线的其他阶段所不能处理的。</p>
<h4 id="2-3-几何处理"><a href="#2-3-几何处理" class="headerlink" title="2.3 几何处理"></a>2.3 几何处理</h4><p>GPU端的几何处理阶段负责大部分的对每个三角形和每个顶点的操作。这个阶段又更深入的分为以下多个阶段: 顶点着色阶段（vertex shading），投影（projection），裁剪（clipping）和屏幕映射(screen mapping)（如图2.3）。</p>
<p><img src="/images/realtime_rendering/chapter2/figure2.3.png" alt=""></p>
<blockquote>
<p>图2.3. 几何阶段按照功能划分为多个阶段的管线。</p>
</blockquote>
<h5 id="2-3-1-顶点着色器"><a href="#2-3-1-顶点着色器" class="headerlink" title="2.3.1 顶点着色器"></a>2.3.1 顶点着色器</h5><p>顶点着色器两个主要的任务，也就是计算顶点的位置和评估程序员期望的任何顶点输出数据，比如法线和贴图坐标。传统上，一个物体的阴影是通过在每个顶点位置和法线应用关照计算出来的，并且只在顶点中保存计算好的颜色。然后在三角形上差值这些颜色（译：因为三角形只有三个顶点存有颜色，三角形面上是没有颜色的，所以面上的颜色通过三个顶点的颜色做差值得到）。因为这些原因，可编程的顶点处理单元被叫做顶点着色器。随着现代GPU的出现，现在部分或者所有的着色器会处理每个像素（译: 像素着色器），顶点着色器阶段可以根据程序员的意图变得更加通用，不计算任何的着色等式。现在的顶点着色器专注于建立每个顶点之间联系的更加通用的单元。比如，使用4.4章节和4.5章节的方法顶点着色器可以让一个对象做动画。</p>
<p>我们从怎样计算出顶点位置开始讲起，一组坐标总是需要的。在渲染到屏幕的过程中，一个模型被转换成多个不同的<em>空间（space）</em>或者<em>坐标系统（coordinate systems）</em>。最开始，一个模型位于自己的<em>模型空间（model space）</em>，这意味着这个模型还没有被做任何的转换。每个模型都可以关联一个<em>模型变换(model transform)</em>以便这个模型可以被定位和定向。在不需要重新生成模型的几何数据的情况下，在同一个场景中允许一个模型有多个副本(也叫实例)，这些副本都各自不同的位置，方向和大小。</p>
<p>模型的顶点和法线都是模型变换（model transform）得到的。对象的坐标叫做<em>模型坐标（model coordinates）</em>，模型坐标通过应用模型变换之后，该模型就被称为位于<em>世界坐标（世界坐标）</em>或者<em>世界空间（world space）</em>（译：其实就是该模型坐标到世界坐标的转换矩阵与该模型坐标相乘，得到的结果就是该模型的世界坐标）。世界坐标是唯一的，随着所有的模型利用他们自己的模型变换来变换自己的模型坐标，这些模型就都处于同一个坐标空间中。</p>
<p>正如前面提到的，只有摄像机可见的那些模型才会被渲染。摄像机在世界空间中有对应的坐标和方向，用来放置和对准摄像机。为了便于投影和裁剪，摄像机和所有的模型都会利用<em>视图变换（view transform）</em>进行转换。视图变换的目的是将摄像机放置于原点并且对准它，使得它看向z轴的负方向，y轴指向向上的方向，x轴指向右方向。我们约定使用-z轴，有些文档喜欢描述它为俯视下的+z轴。区别主要是描述的语义的不同，两者之间很容易转换。在应用了视图变换之后的实际位置和方向取决于低层应用编程接口（API）。这样描绘出来的空间叫做<em>相机空间（camera space）</em>，更通用的称为，<em>视图空间（view space）</em>或者<em>观察空间（eye space）</em>。视图变换对摄像机和模型的影响过程如图2.4。模型变化和视图变换都可以以4x4矩阵来实现，这是第四章的主题。不过，一个重要的认知是程序员可以通过任何喜欢的方式计算得到顶点的位置和法线。</p>
<p><img src="/images/realtime_rendering/chapter2/figure2.4.png" alt=""></p>
<blockquote>
<p>图2.4. 在左边的示意图中，按照用户想法自顶向下显示了相机的位置和方向，在一个+z轴向上的世界里。为了摄像机处于原点，朝向-z轴方向，摄像机的y轴向上，视图变换重新调整了世界的方向，如右图所示。这样做是为了裁剪和投影操作更加简单和快速。淡蓝色的区域是可视空间体。这里假设是一个透视视图，因为这个可视空间体是一个截距体，类似的技术适用于各种投影。</p>
</blockquote>
<p>接下来，我们描述顶点着色器的第二种输出。为了创造出一个逼真的场景，仅仅是渲染对象的形状和位置是不够的，还必须对他们的外观进行建模。外观的建模描述了包括每个对象的材质，以及任何光源照射在对象上的效果。材质和光照从简单的颜色到复杂的物理描述的精细表现可以有很多方法模拟出来。</p>
<p>这种决定材质上光照效果的操作叫做<em>着色（shading）</em>。它涉及对象各个点的着色方程计算。典型的，在几何处理阶段执行的模型的顶点的一些计算，其他的可能在像素处理阶段执行的计算。每个顶点可以存储各种材质的数据，比如点的位置、法线、颜色或者任何其他的需要计算着色公式的数值信息。顶点着色的结果（这个结果可以是颜色，顶点，纹理坐标，连同任何其他的着色数据）接着发送光栅阶段和像素处理阶段分别做差值和进行面（译：因为顶点着色器处理了各个顶点，顶点所连接的面是在像素着色器处理的，像素着色器会处理这些面上的每个像素）上的着色计算。</p>
<p>以GPU顶点着色器的方式进行顶点着色更深入的讨论贯穿了本书，尤其是第3和第5章。</p>
<p>作为顶点着色的一部分，渲染系统执行<em>投影（projection）</em>然后进行裁剪，将可视空间体转换到以（-1，-1，-1）和（1，1，1）为边界点的单元立方体中。相同可视空间体可以定义不同的范围，例如，0 ≤ z ≤ 1。这个单元的立方体叫做<em>标准可视空间体（canonical view volume）</em>。首先在GPU的顶点着色器中处理完投影。通常有两个投影的方式，叫做<em>正交投影（orthographic）（也叫平行）</em>和<em>透视投影（perspective）</em>。如图2.5。</p>
<p><img src="/images/realtime_rendering/chapter2/figure2.5.png" alt=""></p>
<blockquote>
<p>图 2.5. 左边是正交或者平行投影；右边是透视投影。</p>
</blockquote>
<p>事实上，正交投影只是平行投影的一种。还发现了多个有用的其他类型的平行投影，尤其是在算法领域，比如<em>倾斜（oblique）和轴测（axonometric）</em>投影。老式的街机游戏<em>Zaxxon</em>就是以后者命名的。</p>
<p>注意到投影是用矩阵来表示的（4.7节），所以它有时可能和其他的几何转换连在一起计算。</p>
<p>正交视图的可视空间体是一个普通的矩形盒，正交投影将这个可视空间体转换到单元盒。正交投影的最大的特征是变换之后的平行线仍然保持平行。这种转换合并了平移和缩放（译：一个转换的矩阵可以包含平移，缩放和旋转）。</p>
<p>透视投影更加复杂一些，这种类型的投影，位于摄像机越远的对象，投影之后显示的越小。此外，平行线可能在视野中汇聚到一点。因此，透视变换模仿的是我们主观感知对象的大小的方式（译：越远的物体看起越小）。几何上，可视空间体，叫做<em>视椎体（frustum）</em>，是个底部为矩形的平截金字塔。同样的，这个视椎体会被变换到单元盒中。平行和透视投影都可以构造4x4矩阵（第四章）来变换，变换之后，模型被称为在<em>裁剪坐标（clip coordinates）</em>中。这些其实是齐次坐标，将在第4章讨论，并且这些坐标是没有除w的（译：坐标通过（x，y，z，w）表示）。GPU的顶点着色器为了下个功能阶段裁剪正确的工作必须总是输出这种类型的坐标。</p>
<p>尽管这些矩阵把一个包围体变换成另外一个包围体，它们被叫做投影，因为它们显示之后，z坐标不会存储到生成的图片中，但是会被存储在z-buffer中，这些会在2.5章节中介绍。这样，模型就从三维投影到了二位。</p>
<h5 id="2-3-3-顶点处理的可选项"><a href="#2-3-3-顶点处理的可选项" class="headerlink" title="2.3.3 顶点处理的可选项"></a>2.3.3 顶点处理的可选项</h5><p>每个管线都有刚才描述的顶点处理。一旦这个处理完成，有几个可选的可在GPU中处理的阶段，按照这个顺序：<em>曲面细分（tessellation）</em>，几何着色和流式输出。它们的使用依赖硬件的支持（不是所有的GPU都有）和程序员的想法。这些可选阶段彼此独立，并且一般不会通常使用。在第三章中，我们将对每一个可选阶段进行更多的介绍。</p>
<p>第一个可选阶段是曲面细分。想象一下你有一个弹力球对象。如果你用一组三角形来表示它，你可能会遇到质量或者性能问题。你的球在五米外的地方看起来不错，但是靠近每个独立的三角形，尤其是沿着三角形的轮廓，变得可见。如果你用更多的三角形来提高球的质量，当球远距离并且只有少量的像素覆盖在屏幕上时，你可能浪费大量的处理时间和内存。利用曲面细分，使用适当数量的三角形就可以生成一个曲面。</p>
<p>我们已经讨论了一些三角形，但是到目前为止我们仅仅在管线中处理了顶点。三角形可以用来表示点，线和其他对象。顶点可以用来描述曲面，比如一个球。这样的表面可以由一组patch（译：曲面细分使用的一种几何基元类型）指定，每个patch由一组顶点组成。曲面细分阶段本身由一系列阶段组成 ：hull shader、tessellator和domain shader，这些阶段将一组path顶点转换（通常）成一组更大的顶点，然后用这组顶点创建一组新的三角形。场景里的摄像机可以用来决定生成多少三角形：当patch近距离时会场景多一些，当patch处于远处时生成少些。（译：曲面细分阶段可以参考如图E1）</p>
<p><img src="/images/realtime_rendering/chapter2/other2.1.png" alt=""></p>
<blockquote>
<p>图E2.1 ，引用自<a href="https://docs.microsoft.com/en-us/windows/desktop/direct3d11/direct3d-11-advanced-stages-tessellation" target="_blank" rel="external">microsoft doc</a></p>
</blockquote>
<p>接下来一个可选的阶段是<em>几何着色器（geometry shader）</em>。这个着色器要早于曲面细分着色器，所以它在GPU上更为常见一些。像曲面细分着色器一样，几何着色器将各种渲染基元排序然后生成新的顶点。因为创建的范围受限并且输出的基元也受到更多限制，这个阶段非常简单。集合着色器有几个用途，最受欢迎的用途是用来生成粒子（译：粒子特效）。想象下模拟烟花爆炸。每个火球可以用一个点来表示，一个顶点。几何着色器得到每个点并将这些点转换成面向观察者同时覆盖多个像素的正方形（由两个三角形组成），因此为我们着色提供更加有方便的基元。</p>
<p>最后一个可选的阶段叫做<em>流输出（stream output）</em>。这个阶段让我们使用GPU作为一个引擎。不是将我们处理的顶点发送到管线的后续剩余阶段渲染到屏幕上，取而代之的是在这阶段我们可以选择将这些处理的顶点输出到一个数组上做进一步的处理。 这些数据在随后过程中CPU可以使用，或者GPU自己使用。这个阶段典型的用来模拟粒子，像我们的烟花的例子。（译：流输出阶段可以参考图E2）</p>
<p><img src="/images/realtime_rendering/chapter2/other2.2.png" alt=""></p>
<blockquote>
<p>图E2.2 引用自<a href="https://docs.microsoft.com/en-us/windows/desktop/direct3d11/d3d10-graphics-programming-guide-output-stream-stage" target="_blank" rel="external">microsoft doc</a></p>
</blockquote>
<p>这三个阶段按照这个顺序执行：曲面细分，几何着色，流输出。每个都是可选的。不管哪个选项（如果有）被使用，如果我们接着执行管线后续阶段，我们都有一组使用齐次坐标的顶点。这组顶点用来检查渲染顶点是否在相机的观察区域。</p>
<h5 id="2-3-4-裁剪"><a href="#2-3-4-裁剪" class="headerlink" title="2.3.4 裁剪"></a>2.3.4 裁剪</h5><p>只有当一个基元整个或者部分在可视空间体内才需要传递到光栅化阶段（以及随后的像素阶段），然后渲染它们到屏幕上。一个基元完全位于可视空间体内也会被传递到下一个阶段。基元完全处于可视空间体的外部是不会进一步传递，因为它们已经不需要渲染了。部分在可视空间体的基元需要进行裁剪。比如，一个顶点在可视空间体外一个顶点在可视空间体内的直线应该根据可视空间体进行裁剪。这样外部的那个顶点就被会一个位于可视空间体和直线的交点的这个心的顶点所代替。使用投影矩阵变换后的原语依照单元盒（unit cube）的范围被裁剪。在视图转换（view transformation）和投影转换（projection）之前做裁剪的好处是使得裁剪问题的一致性: 原语总是依照单元盒被裁剪。</p>
<p>图2.6描述了裁剪的过程。除了可视空间体的六个裁剪面外，用户可以额外定义剪切平面来显示裁剪对象。图19.1形象的展示了这个类型，叫做<em>切片（sectioning）</em>。</p>
<p>裁剪阶段使用投影生成的四变量的齐次坐标来进行裁剪。值通常不会在透视空间中对三角形做线性差值。需要第四个坐标（译：四变量（x, y, z, w），第四个坐标是w）以便在应用透视投影时数据做差值和裁剪。最终，执行<em>透视划分（perspective division）</em>，将得到的三角形的坐标放置到三维的<em>标准化设备坐标（normalized device coordinates）</em>。之前提到的，可视空间体的范围从（-1，-1，-1）到（1，1，1）。几何阶段的最后一步是从这个空间转换到窗口坐标（译： 模型坐标，观察坐标（或者叫可视坐标），标准化坐标，窗口坐标）。</p>
<p><img src="/images/realtime_rendering/chapter2/figure2.6.png" alt=""></p>
<blockquote>
<p>图2.6 投影转换之后，只有需要继续处理的基元才会到单元盒（对应可视椎体里的基元）中。因此，在单元盒外部的基元就被裁剪掉了，在内部的基元全部保留下来。与单位盒相交的基元会依据单位盒进行裁剪，这样会生成新的顶点同时丢弃旧的顶点。</p>
</blockquote>
<h5 id="2-3-5-屏幕映射（Screen-Mapping）"><a href="#2-3-5-屏幕映射（Screen-Mapping）" class="headerlink" title="2.3.5 屏幕映射（Screen Mapping）"></a>2.3.5 屏幕映射（Screen Mapping）</h5><p>只有在可视空间体内的基元（裁剪过的）才会被传递到屏幕映射阶段，进入这个阶段时坐标还是三维的。每个基元的x坐标和y坐标被变化到<em>屏幕坐标（screen coordinates）</em>。屏幕坐标和z轴坐标也叫做<em>窗口坐标（window coordinates）</em>。假如场景应该渲染到一个最小角的坐标在（x_1, y_1）, 最大角的坐标在（x_2, y_2）的窗口，其中x_1 &lt; x_2并且y_1&lt;y_2。然后屏幕映射通过缩放操作来进行变换。新的x和y坐标叫做屏幕坐标。z坐标（OpenGL为[-1, +1], DirectX为[0, 1]）同样被映射到[z_1, z_2]，z_1 = 0和z_2 = 1作为默认值。不管怎样这些是可以通过API来修改的。窗口以及重新映射的z值被传入光栅化阶段。图2.7描述了屏幕映射的过程。</p>
<p><img src="/images/realtime_rendering/chapter2/figure2.7.png" alt=""></p>
<blockquote>
<p>Figure2.7 基元在投影转换之后位于一个单元盒中，屏幕映射过程负责找到基元在屏幕上的坐标。</p>
</blockquote>
<p>接下来，我们描述下整型和浮点型值与像素（还有纹理坐标）是怎样的关系。给定一个使用笛卡尔坐标的水平像素的数组，最左边的像素的左边的坐标是浮点数0.0。OpenGL一直使用这个方案，DirectX 10已经之后的版本也是使用的这个方案。这个像素的中心是0.5。因此，[0, 9]范围涵盖了值为[0.0, 10.0)的像素。转换方式很简单<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">d = floor(c)</span><br><span class="line">c = d + 0.5</span><br></pre></td></tr></table></figure></p>
<p>此处的<em>d</em>为像素的不连续（译：数学上称为离散）的索引，<em>c</em>（浮点型）是像素中连续的值。</p>
<p>虽然所有的API的像素位置的值都是从左到右递增的，但是位置0处是在顶部还是在底部在OpenGL和DirectX不同的方案上是不一致的。OpenGL一直支持的是笛卡尔坐标系，将左下边的角作为最小值元素，但是DirectX有时是将左上角作为最小值元素，这取决于上下文。每种情况都有其自己的逻辑，不同的情况没有绝对的答案。举个列子，在OpenGL平台（0，0）坐标位于图片的左下角，然而在DirectX平台确实位于图片的左上角。在移植API到其他的平台上时考虑这些不同点非常重要。</p>
<h5 id="2-4-光栅化"><a href="#2-4-光栅化" class="headerlink" title="2.4 光栅化"></a>2.4 光栅化</h5><p>给定转换和投影的顶点以及和顶点关联的着色器数据（均来自几何处理后的数据），下一阶段的目标是在被渲染的基元（比如. 三角形）内找出所有的像素 -（像素：图片元素的简写）。我们称这个过程为<em>光栅化（rasterization）</em>，它分为两个两个功能子阶段：构建三角形（也称为基元组装）和三角形遍历。这些在图Figure2.8左边展示。注意这里也可以处理顶点和线，但是自从三角形更加的普遍，这个子阶段的名字中就有了“三角形”。光栅化，也被叫做<em>扫描变换（scan conversion）</em>，是将二维顶点（每个顶点都有关联的z值（也就是深度值）和各种着色信息）转换到屏幕空间上的像素。光栅化也可以看做是几何处理和像素处理的同步点，因为在这里由三个顶点组成三角形并发送到像素处理。</p>
<p><img src="/images/realtime_rendering/chapter2/figure2.8.png" alt=""></p>
<blockquote>
<p>Figure2.8 左边：光栅化分成两个功能阶段，叫做构建三角形（triangle setup）和遍历三角形（triangle traversal）。右边：像素处理分成两个功能阶段，被称为像素处理（pixel processing）和合并（merging）</p>
</blockquote>
<p>三角形是否被认为是重叠的像素这取决你是怎样设置GPU的管线。比例，你可能使用点采样来确定 “内在性”。最简单的方法是对每个像素的中心点进行采样，如果中心点在三角形内部就那么这个像素就认为是在这个三角形的内部。也可以使用超采样（supersampling）或者多重采样（multisampling）抗锯齿技术（章节5.4.2）对每个像素使用多个采样。另一个方法是使用保守点的光栅化，它的定义是，像素如果在三角形内部那么必须至少有部分和这个三角形重叠（章节23.1.2）。</p>
<h5 id="2-4-1-三角形构建"><a href="#2-4-1-三角形构建" class="headerlink" title="2.4.1 三角形构建"></a>2.4.1 三角形构建</h5><p>在这个阶段计算三角形的微分（differentials），边缘等式和其他的数据。这些数据可能被用于三角形的遍历（章节2.4.2），和几何阶段一样也会差值生成各种着色器的数据。有专门此功能的硬件用于这个任务。</p>
<h5 id="2-4-2-三角形遍历"><a href="#2-4-2-三角形遍历" class="headerlink" title="2.4.2 三角形遍历"></a>2.4.2 三角形遍历</h5><p>在这里会检查每个像素的中心点（或者一个采样）是否被三角形所覆盖，并且会为这部分被三角形覆盖的像素生成一个片元（fragment）。更加详细的采样方法可以在章节5.4中找到。找到哪些采样点或者像素在三角形内部通常被称为<em>三角形遍历（triangle traversal）</em>。每个三角形片元的属性是通过插值三角形的三个顶点来生成的（第5章）。这些属性包括片的深度，同样也有来自几何阶段的所有着色数据。McCormack等[1162]提供了更新关于三角形遍历的信息。仍然在这里会执行三角形上的透视校正插值[694] （章节23.1.1）。原语内的所有像素或者采样被发送到像素处理阶段，接下来描述。</p>
<h4 id="2-5-像素处理"><a href="#2-5-像素处理" class="headerlink" title="2.5 像素处理"></a>2.5 像素处理</h4><p>此时，已经找到的三角形或者其他原语的中考虑的所有像素都是之前所有的阶段组合的结果。像素处理阶段被分为<em>像素着色（pixel shading）</em>和<em>合并（merging）</em>，如图图2.8的右边。像素处理阶段是对原语内的像素和采样执行每个像素或者是每个采样计算和操作的地方。</p>
<h5 id="2-5-1-像素着色器"><a href="#2-5-1-像素着色器" class="headerlink" title="2.5.1 像素着色器"></a>2.5.1 像素着色器</h5><p>这里使用插值的着色数据作为输入，逐像素执行着色计算。最终会产生一个或多个颜色被传入到下一个阶段。不像三角形构建和三角形遍历，通常是由专门的硅晶体硬件执行，像素着色器阶段是通过可编程的GPU核执行的。为此，程序开发者提供一个像素着色器程序（或者是片元着色器，在OpenGL是这样的），它可以包含任何想要的计算。这里可以使用多种技术，其中最重要的是<em>纹理化（texturing）</em>。 纹理化将在第6章更加详细的讨论。简单来说，为了各种目的，纹理化一个对象意味着将一张或者多张图片粘合（gluing）在这个对象上。一个简单的例子如图2.9所示。这个图片可能是一维，二维或者是三维的，通常是使用二维的图片。简而言之，每个片元最终生成的是一个颜色值，这些颜色值会被传入下个子阶段。</p>
<p><img src="/images/realtime_rendering/chapter2/figure2.9.png" alt=""></p>
<blockquote>
<p>图2.9  左上方显示一个没有纹理的龙模型。图片纹理块是粘合在龙上的，粘合后的结果显示在左下方。</p>
</blockquote>
<h5 id="2-5-2-合并"><a href="#2-5-2-合并" class="headerlink" title="2.5.2 合并"></a>2.5.2 合并</h5><p>每个像素的信息存储在<em>颜色缓冲（color buffer）</em>中，它是一个矩形的颜色数组（每个颜色由红色，绿色和蓝色组成），合并阶段的责任将像素着色阶段生成的片元颜色和当前存储在缓冲区的像素进行合并。这个阶段也叫ROP，表示“光栅操作（raster operations）（管线）”或者“渲染输出单元（render output unit）”，这取决你问谁。不像着色阶段，执行这个阶段的GPU的子单元通常不是完全可编程的。可是，它是高度可配置，支持各种效果。</p>
<p>这个阶段还负责解决可见性问题。这意味着，当渲染场景时，颜色缓冲应该包含场景中从摄像机位置可见的原语的颜色。对于大多数甚至所有的图形，这是通过z-buffer算法来完成的（也成为深度缓存）。深度缓冲（z-depth）和颜色缓冲有同样的大小和形状（shape），每个像素存储z值到最邻近的原语。这意味着当一个原语被渲染到一个明确的像素时，会计算原语上这个像素的z值，并且和相同像素的深度缓冲的内容进行比较。如果新的z值比深度缓冲里面的小，那么这个被渲染的原语比之前这个像素最靠近摄像机的原语更靠近摄像机。因此，这个像素的z值和颜色更新为被绘制出来的原语的这个像素的z值和颜色。如果计算出来的z值大于深度缓冲中的z值，颜色缓冲和深度缓冲将保持不变。深度缓冲的算法很简单，复杂度为O(n)（n是渲染的原语的数量），适用于任何绘制原语计算每个（相关）像素的z值。还需要注意这个算法允许大多数原语以任何顺序进行渲染，这是它受欢迎的另外一个原因。但是，深度缓冲仅仅存储了屏幕上每个点的单个深度值，所以它不能用于部分透明原语。透明原语必须在所有的非透明原语渲染之后再渲染，并且从后往前渲染，或者是使用单独的与顺序无关的算法（第5.5章节）。渲染透明度原语是基础的深度缓冲的主要的弱点。</p>
<p>我们已经提到，颜色缓冲是用来存储每个像素的颜色的，深度缓冲是用来存储每个像素的深度值（z-value）的。此外，还有其他的通道和缓冲区可以用来过滤和捕获片元信息。<em>alpha通道</em>和颜色缓冲相关联，为每个像素存储相关的非透明度（第5.5章节）。在旧的API中，alpha通道被用来根据alpha测试特性来选择性的裁剪（discard）像素。现在裁剪操作可以插入到像素着色器程序，并且任何类型的计算都能用于出发裁剪。这种类型的测试用来确保完全透明的片元不会影响深度缓冲（第6.6章节）。</p>
<p><em>模版缓冲（stencil buffer）</em>是一个离屏的缓冲，它被用来记录渲染原语的位置。通常每像素包含8 bits，原语可以使用各种函数渲染到模版缓冲中，并且这个缓冲的内容可以用来控制渲染到颜色缓冲和深度缓冲。举个例子，假设在模版缓冲区填充一个圆。这可以与一个操作符合并（译：位运算），根据这个合并结果允许只有保留圆的地方渲染后续原语到颜色缓冲。模版缓冲在生成一些特殊方面是一个非常强大的工具。所有这些管线末端的功能叫做<em>光栅化操作（raster operations）（ROP）</em>或者 <em>混合操作（blend operations）</em>。可以将当前缓冲区的颜色与正在处理的三角形内部的像素颜色进行混合。这里可以实现一些如透明度或者颜色样本积累之类的效果。正如上面提到的，混合（blending）是可以使用API配置的，并不是完全可编程的。然而，一些API是支持光栅化顺序视图，也成为像素着色器顺序，它是支持可编程混合的。</p>
<p><em>framebuffer</em>通常是由系统上的所有缓冲组成的。</p>
<p>当原语到达并且通过光栅化阶段，那些在摄像机视图中可见的点被显示在屏幕上。屏幕上显示颜色缓冲的内容。为了避免观察者看到原语在被光栅化以及发送光栅化的结果到屏幕的这个过程，<em>双缓冲（double buffering）</em>被应用。这意味着渲染一个场景发生在离屏的<em>后置缓冲（back buffer）</em>中。一旦场景渲染完成到后置缓冲中，后置缓冲的内容就与先前显示在屏幕的<em>前置缓冲（front buffer）</em>的内容进行交换。交换通常发生在<em>垂直折回（vertical retrace）</em>（译：显示器的刷新时从左上角逐行到右下角的，垂直折回就是从右下角回到左上角）时期，此时是比较安全的。</p>
<p>更多关于缓冲区和缓冲的方法，详见 第5.4.2，23.6和23.7章节</p>
<h4 id="2-6-整个管线"><a href="#2-6-整个管线" class="headerlink" title="2.6 整个管线"></a>2.6 整个管线</h4><p>点，线和三角形是构成一个模型或者对象的渲染原语。想象下假如应用程序是交互式的CAD程序，用户正在测试华夫饼机器的设计。这里我们将随着这个模型通过整个图形渲染管线，由四个主要的步骤：应用，几何，光栅和像素处理。这个场景以透视投影渲染到一个屏幕的窗口中。在这个简单的例子中，这个华夫饼模型包括线（显示各部分的边缘）和三角形（显示表面）。这台华夫饼机器有一个可以打开的盖子。其中一些三角形使用带有制造商商标的二维图片构成的纹理。还是这个例子，除了发生在光栅化阶段的纹理应用，表面着色完全在几何阶段计算的。</p>
<h5 id="2-6-1-应用"><a href="#2-6-1-应用" class="headerlink" title="2.6.1 应用"></a>2.6.1 应用</h5><p>CAD应用允许用户选择和移动模型的各个部分。比如，用户可能选择这个盖子然后移动鼠标打开它。应用阶段必须转换鼠标的移动为相应的旋转矩阵，然后当这个盖子被渲染时，确保这个矩阵被正确的应用到这个盖子。另外一个例子：一个播放动画是根据事先定义好的路径移动相机，来显示不同视角的华夫饼机器。相机的参数，比如位置和视角方向，随着时间必须在应用中更新。为了渲染每一帧，应用阶段获取相机的位置，光照和模型的原语提供给管线下一个主要的阶段-几何阶段。</p>
<h5 id="2-6-2-几何阶段"><a href="#2-6-2-几何阶段" class="headerlink" title="2.6.2 几何阶段"></a>2.6.2 几何阶段</h5><p>对于透视视图，我们假设到这里应用层已经提供了透视矩阵。而且，对于每个对象，应用层计算好了一个矩阵，这个矩阵描述了视图转换以及对象本身的位置和方向。在我们的例子中，华夫饼制造商基地有一个矩阵，机器的盖子有另外一个。在几何阶段对象的顶点和法线利用这个矩阵进行变换，把对象放置到视图空间。然后可以使用材质和光源的属性来对顶点进行着色或者其他的计算。接着使用用户单独提供的投影矩阵来执行投影变换，将用户可以看到的对象变换到一个单元盒空间，所有在单元盒空间外的原语被丢弃。为了获得一组都在单元盒内部的原语，所有和单元盒相交的原语会依据单元盒来进行裁剪。然后这些顶点映射到屏幕的一个窗口内。在执行了每个三角形和每个顶点操作之后，结果数据被传递到光栅化阶段。</p>
<h5 id="2-6-3-光栅化"><a href="#2-6-3-光栅化" class="headerlink" title="2.6.3 光栅化"></a>2.6.3 光栅化</h5><p>接着对所有在前一个阶段被裁减的幸存者进行光栅化，这意味着找到原语内部的所有的像素并且发送到管线的下一步去做像素处理。</p>
<h5 id="2-6-4-像素处理"><a href="#2-6-4-像素处理" class="headerlink" title="2.6.4 像素处理"></a>2.6.4 像素处理</h5><p>这一步的目标是计算每个可见的原语的每个像素的颜色。那些与任何纹理（图片）关联的三角形使用应用它们期望的图片来渲染。可见性通过深度缓冲（z-buffer）算法以及选择性丢弃和模板测试来解决。每个对象依次处理，最终图像显示在屏幕上。</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>这个管线的成果来自数十年的API和图形硬件对实时渲染程序发展的目标。需要注意的是，这并不是唯一的渲染管线。离线渲染管线存经历了不同的发展的路径。电影工业渲染通常使用的是<em>微多边形（micropolyon）</em>管线，但是射线跟踪和路径跟踪已经开始流行起来。这些技术在11.2.2章节中讲述，也可以用于建筑设计和视觉设计。</p>
<p>多年来，开发者使用这里描述的流程的唯一方法是通过使用图形API里面定义的<em>固定函数管道（fixed-function pipeling）</em>。固定函数管道正如其名，因为实现图形硬件的组成部分是不能够使用灵活的途径编程。最后一个主要的固定函数的机器是任天堂的Wii，2006年推出。可编程的GPU，另一方面来讲，使得可以准确的确定整个管线每个子阶段的应用了哪些操作成为可能。在这本书的第四版中，我们假设所有的开发者都是的可编程的GPU。</p>
<p><em>END</em></p>

      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/graphics/" rel="tag"># graphics</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/04/22/graphics/mesh_optimizer/" rel="next" title="Unity Mesh合并工具">
                <i class="fa fa-chevron-left"></i> Unity Mesh合并工具
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  
    <div class="comments" id="gitalk-container">
    </div>

  



        </div>
        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">yiliangduan@gmail.com</span>

  

  
</div>









        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/utils.js?v=7.1.0"></script>

  <script src="/js/motion.js?v=7.1.0"></script>



  
  


  <script src="/js/schemes/muse.js?v=7.1.0"></script>




  
  <script src="/js/scrollspy.js?v=7.1.0"></script>
<script src="/js/post-details.js?v=7.1.0"></script>



  


  <script src="/js/next-boot.js?v=7.1.0"></script>


  

  

  

  


  
    

<script src="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>



<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"/>



<script src="//cdn.jsdelivr.net/npm/js-md5@0.7.3/src/md5.min.js"></script>

<script>
  var gitalk = new Gitalk({
    clientID: '6bdf275ac612580f41fe',
    clientSecret: 'b9415c3424b9f70954e5493998d95d58559d36ee',
    repo: 'yiliangduan.github.com',
    owner: 'yiliangduan',
    admin: ['yiliangduan'],
    id: md5(location.pathname),
    
      language: window.navigator.language || window.navigator.userLanguage,
    
    distractionFreeMode: 'true'
  });
  gitalk.render('gitalk-container');
</script>

  


  




  

  

  

  

  

  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


  

  

  

  

  

  

  

  

</body>
</html>
